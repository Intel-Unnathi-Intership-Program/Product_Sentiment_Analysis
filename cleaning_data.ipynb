{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKkSSVn/6pclLoHpTkUUWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Intel-Unnathi-Intership-Program/Product_Sentiment_Analysis/blob/main/cleaning_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZWXodOx1lyD8"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Ensure the required NLTK data packages are downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the dataset\n",
        "file_path = r\"C:\\Users\\Ananya\\Documents\\Intel Dataset\\raw_dataset.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Display the first few rows of the dataset\n",
        "print(\"Original Data:\")\n",
        "print(df.head())\n",
        "\n",
        "# Data Cleaning Steps\n",
        "\n",
        "# 1. Handling missing values\n",
        "# Drop rows with any missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# 2. Removing duplicates\n",
        "# Drop duplicate rows\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# 3. Normalizing text (assuming there's a 'review_body' column)\n",
        "# Convert column to string type to handle non-string values\n",
        "df['review_body'] = df['review_body'].astype(str)\n",
        "\n",
        "# Convert text to lowercase\n",
        "df['review_body'] = df['review_body'].str.lower()\n",
        "\n",
        "# Remove punctuation and special characters\n",
        "df['review_body'] = df['review_body'].str.replace(r'[^\\w\\s]', '', regex=True)\n",
        "\n",
        "# Remove numbers\n",
        "df['review_body'] = df['review_body'].str.replace(r'\\d+', '', regex=True)\n",
        "\n",
        "# Initialize the WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove \"read more\"\n",
        "    text = text.replace('read more', '')\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    words = [lemmatizer.lemmatize(word, pos='v') for word in words]  # Apply lemmatization\n",
        "    return ' '.join(words)\n",
        "\n",
        "df['review_body'] = df['review_body'].apply(clean_text)\n",
        "\n",
        "# Temporarily save the cleaned dataset to a new file\n",
        "temp_file_path = r\"C:\\Users\\Ananya\\Documents\\Intel Dataset\\dataset.csv\"\n",
        "df.to_csv(temp_file_path, index=False)\n",
        "\n",
        "# Display the first few rows of the cleaned dataset\n",
        "print(\"Cleaned Data:\")\n",
        "print(df.head())"
      ]
    }
  ]
}